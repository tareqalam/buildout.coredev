#!/usr/bin/env python3
"""A concurrent wrapper for timing xmltestrunner tests for buildout.coredev."""
from argparse import ArgumentParser
from fnmatch import fnmatch
from itertools import cycle
from logging import Formatter
from logging import getLogger
from logging import INFO
from logging import StreamHandler
from logging.handlers import MemoryHandler
from multiprocessing import cpu_count
from multiprocessing import Pool
from os import access
from os import environ
from os import killpg
from os import path
from os import pathsep
from os import setpgrp
from os import unlink
from os import walk
from os import X_OK
from signal import SIGINT
from signal import SIGKILL
from signal import signal
from subprocess import check_output
from subprocess import DEVNULL
from subprocess import PIPE
from subprocess import Popen
from time import sleep
from time import time
import locale
import re
import sys


def which(program):
    def is_exe(fpath):
        return path.isfile(fpath) and access(fpath, X_OK)

    fpath, fname = path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for dpath in environ["PATH"].split(pathsep):
            exe_file = path.join(dpath, program)
            if is_exe(exe_file):
                return exe_file

    return None


def humanize_time(seconds):
    """Humanize a seconds based delta time.

    Only handles time spans up to weeks for simplicity.
    """
    minutes, seconds = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    days, hours = divmod(hours, 24)
    weeks, days = divmod(days, 7)

    seconds = int(seconds)
    minutes = int(minutes)
    hours = int(hours)
    days = int(days)
    weeks = int(weeks)

    output = []

    if weeks:
        quantifier = 'weeks' if weeks > 1 or weeks == 0 else 'week'
        output.append('{} {}'.format(weeks, quantifier))
    if days:
        quantifier = 'days' if days > 1 or days == 0 else 'day'
        output.append('{} {}'.format(days, quantifier))
    if hours:
        quantifier = 'hours' if hours > 1 or hours == 0 else 'hour'
        output.append('{} {}'.format(hours, quantifier))
    if minutes:
        quantifier = 'minutes' if minutes > 1 or minutes == 0 else 'minute'
        output.append('{} {}'.format(minutes, quantifier))

    quantifier = 'seconds' if seconds > 1 or seconds == 0 else 'second'
    output.append('{} {}'.format(seconds, quantifier))

    return ' '.join(output)


def setup_termination():
    # Set the group flag so that subprocesses will be in the same group.
    setpgrp()

    def terminate(signum, frame):
        # Kill the group (including main process) on terminal signal.
        killpg(0, SIGKILL)

    signal(SIGINT, terminate)


def discover_tests():
    logger.info('Discovering tests: %s', ', '.join(TEST_SUITES))

    batches = {}

    pool = Pool(CONCURRENCY)
    test_discovery = pool.map(fetch_test_discovery_output, TEST_SUITES.items())

    pool.close()
    pool.join()

    for result in test_discovery:
        suite, output = result
        batches[suite] = {}
        group = None
        layer = None
        classname = None
        for line in output:
            if line.startswith('#### Running'):
                group = re.search('^#### Running tests for (.*) ####$', line).groups()[0]
                batches[suite][group] = {}

            if group and line.startswith('Listing'):
                layer = re.search('^Listing (.*) tests:', line).groups()[0]
                batches[suite][group][layer] = {}

            # All listed tests are indented with 2 spaces
            if group and layer and line.startswith('  '):
                if '(' in line:
                    classname = re.search('.*\((.*)\).*', line).groups()[0]
                else:
                    # Some doctests have a filename:testcase convention
                    classname = re.search('([^:]*)', line).groups()[0].strip()

                # Count discovered tests per testclass
                if not batches.get(suite).get(group).get(layer).get(classname):
                    batches[suite][group][layer][classname] = 0

                batches[suite][group][layer][classname] += 1

    return batches


def fetch_test_discovery_output(discovery_job):
        jobname, jobspec = discovery_job
        job_env = jobspec.get('ENV')
        cmdline = jobspec.get('CMDLINE')
        env = environ.copy()

        if job_env:
            for key, value in job_env.items():
                env[key] = value

        output_encoding = sys.stdout.encoding

        if output_encoding is None:
            output_encoding = locale.getpreferredencoding()

        process = Popen(
            cmdline,
            env=env,
            stderr=DEVNULL,
            stdout=PIPE,
            )

        stdout = process.communicate()[0]
        return (jobname, stdout.decode(output_encoding).splitlines(), )


def split(batch, maxcount=512, robot_maxcount=8):
    suite = batch.get('suite')
    if suite == 'ROBOT':
        maxcount = robot_maxcount
    batch['count'] = sum(batch.get('testclasses').values())
    if batch.get('count') <= maxcount or len(batch.get('testclasses')) == 1:
        return (batch, )

    sorted_testclasses = sorted(
        batch.get('testclasses').items(),
        key=lambda testclass: -testclass[1],
        )

    split_batches = [
        {'suite': suite, 'group': batch.get('group'), 'layer': batch.get('layer'), 'testclasses': {}, 'original_count': batch.get('original_count')},
        {'suite': suite, 'group': batch.get('group'), 'layer': batch.get('layer'), 'testclasses': {}, 'original_count': batch.get('original_count')},
        ]

    alternator = cycle((0, 1, ))

    while sorted_testclasses:
        testclass = sorted_testclasses.pop(0)
        split_batches[next(alternator)]['testclasses'][testclass[0]] = testclass[1]

    return tuple(sorted(
        (a for b in split_batches for a in split(b)),
        key=lambda batch: -batch.get('count'),
        ))


def create_test_run_params():
    test_run_params = []

    for suite, groups in discover_tests().items():
        for group, layers in groups.items():
            for layer, testclasses in layers.items():
                batch = {}
                batch['suite'] = suite
                batch['group'] = group
                batch['layer'] = layer
                batch['testclasses'] = testclasses
                batch['original_count'] = sum(testclasses.values())
                split_batches = split(batch)
                for i, split_batch in enumerate(split_batches):
                    split_batch['batchinfo'] = '{}/{}'.format(i + 1, len(split_batches))
                    test_run_params.append(split_batch)

    return tuple(sorted(
        test_run_params,
        key=lambda batch: (-batch.get('original_count'), -batch.get('count'), ),
        ))


def remove_bytecode_files(directory_path):
    logger.info('Removing bytecode files from %s', directory_path)

    for filename in find_bytecode_files(directory_path):
        unlink(filename)


def find_bytecode_files(directory_path):
    for root, _, files in walk(directory_path):
        for name in files:
            if fnmatch(name, '*.py[co]'):
                yield path.join(root, name)


def run_tests(test_run_params):
    """Run and time 'bin/test --layer layer -m module [-m module]'.

    Return the module name, layer name and stderr.
    """

    params = ['bin/test']

    suite = test_run_params.get('suite')

    if suite == 'ROBOT':
        params.append('--all')

    params.append('--xml')

    group = test_run_params.get('group')
    layer = test_run_params.get('layer')
    batchinfo = test_run_params.get('batchinfo')
    testclasses = test_run_params.get('testclasses', ())
    count = test_run_params.get('count')

    if layer:
        params.append('--layer')
        params.append(layer)

    for testclass in testclasses:
        params.append('-t')
        params.append(testclass)

    printable_params = ' '.join(["'{}'".format(param) if ' ' in param else param for param in params])

    logger.info(
        "START - %s - %s - %s %s - %d %s",
        suite,
        group,
        layer,
        batchinfo,
        count,
        'test' if count == 1 else 'tests',
        )

    # Explicitly flush to trigger IPC over cPickle
    memory_handler.flush()

    env = environ.copy()

    # Somehow these globals do not seem to get pickled, no idea what's up there
    # Should be able to just:
    # env['ROBOT_BROWSER'] = TEST_SUITES.get(suite).get('ENV').get('ROBOTSUITE_PREFIX', '')
    # env['ROBOTSUITE_PREFIX'] = TEST_SUITES.get(suite).get('ENV').get('ROBOT_BROWSER', '')
    if suite == 'ROBOT':
        if 'xvfb-run' in TEST_SUITES['ROBOT']['CMDLINE']:
        xvfb_params = [
            'xvfb-run',
            '-a',
            "--server-args='-screen 0 1920x1200x24'",
        ]
        params = tuple(xvfb_params + params)
        env['ROBOTSUITE_PREFIX'] = 'ONLYROBOT'
        # Chromium > Firefox
        if which('chromedriver'):
            env['ROBOT_BROWSER'] = 'chrome'
        elif which('geckodriver'):
            env['ROBOT_BROWSER'] = 'firefox'

    start = time()

    process = Popen(
        params,
        env=env,
        stderr=PIPE,
        stdout=PIPE,
        )

    stdout, stderr = process.communicate()
    returncode = process.returncode

    runtime = time() - start

    result = {
        'suite': suite,
        'group': group,
        'layer': layer,
        'returncode': returncode,
        'runtime': runtime,
        'stderr': stderr,
        }

    done_args = (
        'DONE - %s - %s - %s %s - %d %s in %s',
        suite,
        group,
        layer,
        batchinfo,
        count,
        'test' if count == 1 else 'tests',
        humanize_time(runtime),
        )

    if returncode:
        logger.error(*done_args)
    else:
        logger.info(*done_args)

    # Explicitly flush to trigger IPC over cPickle (and to avoid
    # carryover-fragment-via-pickling)
    memory_handler.flush()

    if returncode:
        log_output.error('')
        log_output.error('Command line')
        log_output.error('')
        log_output.error(printable_params)
        log_output.error('')

        if stderr:
            log_output.error('STDERR')
            log_output.error('')
            for line in stderr.decode('UTF-8').splitlines():
                log_output.error(line)
            log_output.error('')

        if returncode:
            log_output.error('STDOUT')
            log_output.error('')
            for line in stdout.decode('UTF-8').splitlines():
                log_output.error(line)
            log_output.error('')

    elif stderr:
        log_output.info('')
        log_output.info('Command line')
        log_output.info('')
        log_output.info(printable_params)
        log_output.info('')

        log_output.info('STDERR')
        log_output.info('')
        for line in stderr.decode('UTF-8').splitlines():
            log_output.error(line)
        log_output.info('')

        # Explicitly flush to trigger IPC over cPickle (and to avoid
        # carryover-fragment-via-pickling)
        stdout_handler.flush()

    return result


def handle_results(results):
    failed_tests = []
    unclean_stderr = []
    job_count = len(results)
    runtime = 0

    while results:
        sleep(1)

        mature_indices = []
        mature_results = []

        for i, result in enumerate(results):
            if result.ready():
                mature_indices.append(i)
                # The enumeration index can be off for the .pop() otherwise
                break

        for i in mature_indices:
            mature_results.append(results.pop(i).get())

        for result in mature_results:
            returncode = result.get('returncode', 1)
            stderr = result.get('stderr')
            runtime += result.get('runtime')

            if returncode:
                failed_tests.append((result.get('suite'), result.get('group'), result.get('layer'), ))

            if stderr:
                unclean_stderr.append((result.get('suite'), result.get('group'), result.get('layer'), ))

    error_count = len(failed_tests)

    if error_count:
        logger.error('%d / %d jobs failed.', error_count, job_count)

        for result in set(failed_tests):
            suite, group, layer = result
            logger.error('Failures in %s %s %s', suite, group, layer)

    stderr_count = len(unclean_stderr)

    if stderr_count:
        logger.info('%d / %d jobs had an unclean STDERR.', stderr_count, job_count)

        for result in set(unclean_stderr):
            suite, group, layer = result
            logger.info('Unclean STDERR in %s %s %s', suite, group, layer)

    logger.info(
        'Aggregate runtime %s.',
        humanize_time(runtime)
        )

    return len(failed_tests) == 0


def main():
    """Discovers and times tests in parallel via multiprocessing.Pool()."""
    # Remove *.py[co] files to avoid race conditions with parallel workers
    # stepping on each other's toes when trying to clean up stale bytecode.
    #
    # Setting PYTHONDONTWRITEBYTECODE is not enough, because running buildout
    # also already precompiles bytecode for some eggs.
    remove_bytecode_files(SOURCE_PATH)

    start = time()
    test_run_params = create_test_run_params()
    logger.info('Discovered tests in %s', humanize_time(time() - start))
    logger.info('Split the tests into %d jobs', len(test_run_params))
    logger.info('Running the jobs in up to %d processes in parallel', CONCURRENCY)

    # We need to explicitly flush here in order to avoid multiprocessing
    # related log output duplications due to picking inputs and globals as the
    # default IPC mechanism
    memory_handler.flush()

    start = time()

    pool = Pool(CONCURRENCY)

    results = []

    for params in test_run_params:
        # Alleviate cPickle load bursting for IPC
        sleep(0.25)
        results.append(pool.apply_async(run_tests, (params, )))

    success = handle_results(results)

    pool.close()
    pool.join()

    logger.info('Wallclock runtime %s.', humanize_time(time() - start))

    if success:
        logger.info('No failed tests.')
        return True

    return False


# Having the __main__ guard is necessary for multiprocessing.Pool().
if __name__ == '__main__':
    # Globals
    environ['PYTHONUNBUFFERED'] = '1'
    environ['PYTHONDONTWRITEBYTECODE'] = '1'

    CONCURRENCY = int(environ.get('MTEST_PROCESSORS', cpu_count()))
    BUILDOUT_PATH = path.abspath(path.join(__file__, '..', '..'))
    SOURCE_PATH = path.join(BUILDOUT_PATH, 'src')

    # CLI arguments
    parser = ArgumentParser(
        description='Run tests in parallel.',
        epilog='At least one of --dx or --robot is required. '
        'You must also have either chromedriver / chromium '
        'or geckodriver / firefox in your $PATH to run Robot tests.',
        )

    parser.add_argument(
        '--at',
        action='store_true',
        help="Run Achetype tests.",
        )

    parser.add_argument(
        '--dx',
        action='store_true',
        help="Run Dexterity tests.",
        )

    parser.add_argument(
        '--robot',
        action='store_true',
        help="Run Robot tets.",
        )

    parser.add_argument(
        '-j',
        '--jobs',
        type=int,
        help='Set the testing concurrency level. '
        'Defaults to the number of threads.',
        )

    args = parser.parse_args()

    if not any((args.at, args.dx, args.robot, )):
        parser.print_help()
        exit(1)

    if args.jobs:
        CONCURRENCY = int(args.jobs)

    TEST_SUITES = {}

    if args.at:
        TEST_SUITES['AT'] = {}
        TEST_SUITES['AT']['ENV'] = {}
        TEST_SUITES['AT']['CMDLINE'] = ('bin/alltests-at', '--list-tests', )

    if args.dx:
        TEST_SUITES['DX'] = {}
        TEST_SUITES['DX']['ENV'] = {}
        TEST_SUITES['DX']['CMDLINE'] = ('bin/alltests', '--list-tests', )

    if args.robot:
        TEST_SUITES['ROBOT'] = {}
        TEST_SUITES['ROBOT']['ENV'] = {'ROBOTSUITE_PREFIX': 'ONLYROBOT'}
        # Chromium > Firefox
        if which('chromedriver'):
            TEST_SUITES['ROBOT']['ENV']['ROBOT_BROWSER'] = 'chrome'
        elif which('geckodriver'):
            TEST_SUITES['ROBOT']['ENV']['ROBOT_BROWSER'] = 'firefox'

        if not TEST_SUITES.get('ROBOT').get('ENV').get('ROBOT_BROWSER'):
            parser.print_help()
            exit(1)

        TEST_SUITES['ROBOT']['CMDLINE'] = ['bin/alltests', '-t', 'ONLYROBOT', '--all', '--list-tests']
        if args.robot and which('xvfb-run'):
            TEST_SUITES['ROBOT']['CMDLINE'] = ['xvfb-run', '-a'].extend(TEST_SUITES.get('ROBOT').get('CMDLINE'))
        TEST_SUITES['ROBOT']['CMDLINE'] = tuple(TEST_SUITES.get('ROBOT').get('CMDLINE'))

    default_loglevel = INFO

    # Logging
    logger = getLogger('mtest')
    logger.setLevel(default_loglevel)

    # Set up logging to stdout
    stream_handler = StreamHandler()
    stream_handler.setLevel(default_loglevel)
    log_formatter = Formatter(
        ' - '.join((
            '%(asctime)s',
            '%(levelname)s',
            '%(message)s',
            )),
        )
    stream_handler.setFormatter(log_formatter)
    # Buffer log messages so we do not get broken-by-racecondition lines
    memory_handler = MemoryHandler(2, target=stream_handler)
    memory_handler.setLevel(default_loglevel)
    logger.addHandler(memory_handler)

    # Set up a separate logger for writing failure output to stdout. We do this
    # because the logging module handles I/O encoding properly, whereas with
    # 'print' we'd need to do it ourselves. (Think piping the output of
    # bin/mtest somewhere, or shell I/O redirection).
    log_output = getLogger('mtest.output')
    log_output.propagate = False
    stdout_handler = StreamHandler(stream=sys.stdout)
    stdout_handler.setFormatter(Formatter(''))
    log_output.addHandler(stdout_handler)
    log_output.setLevel(INFO)

    setup_termination()

    if args.robot:
        logger.info('Autodetected Robot browser: %s', TEST_SUITES.get('ROBOT').get('ENV').get('ROBOT_BROWSER'))
        if 'xvfb-run' in TEST_SUITES.get('ROBOT').get('CMDLINE'):
            logger.info("Using 'xvfb-run -a' to wrap the robot tests.")
        else:
            logger.info("Not using 'xvfb-run -a' to wrap the robot tests.")

    if main():
        exit(0)

    exit(1)
